{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramaters\n",
    "BATCH = 128\n",
    "MAX_EPOCH = 10\n",
    "NOISE_SIZE = 100\n",
    "#C = 3 # initial channel size for discriminator\n",
    "lr = 0.0002 # Learning rate for Adam Optimiser\n",
    "beta1 = 0.5 # momentum term for Adam Optimiser\n",
    "beta2 = 0.9\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "        if m.bias is not None: #if whole value is not None\n",
    "            m.bias.data.zero_()\n",
    "        \n",
    "#biases to zero, weights to normal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen.conv_t.weight.data.normal_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader \n",
    "\n",
    "# load data\n",
    "# test one output and visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for Generator\n",
    "\n",
    "class Gen(nn.Module): \n",
    "    def __init__(self, noise_size):\n",
    "        super(Gen, self).__init__()\n",
    "        # create all layers\n",
    "        self.conv_t =  nn.ConvTranspose2d(noise_size, 1024, 4, stride=1, padding=0, bias=False) \n",
    "        self.conv_t1 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False) \n",
    "        self.conv_t2 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False) \n",
    "        self.conv_t3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv_t4 = nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(1024)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(3)\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param torch.Tensor z: random noise vector [B, N]\n",
    "        :returns: output image [B, 3, 64, 64]\n",
    "        \"\"\"\n",
    "        # Pass through each layer \n",
    "        # [B, N] -> [B, 1024, 4, 4]\n",
    "        x = F.relu(self.bn1(self.conv_t(z)))\n",
    "        # [B, 1024, 4, 4] -> [B, 512, 8, 8]\n",
    "        x = F.relu(self.bn2(self.conv_t1(x)))\n",
    "        # [B, 512, 8, 8] -> [B, 256, 16, 16]\n",
    "        x = F.relu(self.bn3(self.conv_t2(x)))\n",
    "        # [B, 256, 16, 16] -> [B, 128, 32, 32]\n",
    "        x = F.relu(self.bn4(self.conv_t3(x)))\n",
    "        # [B, 128, 32, 32] -> [B, 3, 64, 64]\n",
    "        x = torch.tanh((self.conv_t4(x))) # images will be between -1 and 1 with tanh. NO BATCH NORM ON OUTPUT\n",
    "        return x\n",
    "\n",
    "# instanciate model\n",
    "gen = Gen(NOISE_SIZE)\n",
    "\n",
    "\n",
    "#print(torch.std(gen.bn1.bias)) #check via std value for conv, then for bias use min\n",
    "# initialise model weights: zero-centred Normal Distr. with std dev = 0.02\n",
    "gen.apply(weights_init)\n",
    "#print(torch.std(gen.bn1.bias)) #as above\n",
    "\n",
    "# test on random inputs\n",
    "z = torch.randn(BATCH, NOISE_SIZE, 1, 1)\n",
    "\n",
    "#output = model.forward(z) #another way of calling your Gen() model\n",
    "outputG = gen(z)\n",
    "\n",
    "# output = model(z)\n",
    "print(outputG.shape)\n",
    "\n",
    "# discriminator\n",
    "# input will be output of Gen, so you know the size/dimensions. Need to figure out what the output of the first layer of discr will be\n",
    "# then figure out how to instantiate those layers, so maybe strided conv? Keep in mind you still use Batch Norm and ReLU, they don't change\n",
    "# they just affect how well your layer/training performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Discriminator\n",
    "\n",
    "class Discr(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Discr, self).__init__()\n",
    "        # Create all layers\n",
    "        self.conv  = nn.Conv2d(3, 128, 4, stride=2, padding=1, bias=False) #first layer, final output of original Gen, no Transpose\n",
    "        self.conv1 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(1024, 1, 4, stride=2, padding=0, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(128) #capital B because it's part of the Class\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.bn4 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # [B, 3, 64, 64] -> [B, 128, 32, 32]\n",
    "        x = (self.conv(image)) # CHECK WHETHER BN IS REQUIRED -> BN REMOVED\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = self.bn2(self.conv1(x))\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = self.bn3(self.conv2(x))\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "\n",
    "        x = self.bn4(self.conv3(x))\n",
    "        x = F.leaky_relu(x, negative_slope=0.2)\n",
    "        # [B, 1024, 4, 4] -> [B, 1]\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), 1)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# instantiate model\n",
    "dis = Discr()\n",
    "\n",
    "#print(torch.std(dis.conv.weight))\n",
    "#Initialise model weights for discriminator: dis.apply\n",
    "dis.apply(weights_init)\n",
    "#print(torch.std(dis.conv.weight))\n",
    "# test on input\n",
    "image = torch.randn(BATCH, 3, 64, 64)\n",
    "\n",
    "# output \n",
    "outputD = dis(image)\n",
    "#print(outputD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize ciriterion (loss function) and 2 optimizer \n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimiserG = torch.optim.Adam(gen.parameters(), lr, (beta1,beta2)) #Generator\n",
    "optimiserD = torch.optim.Adam(dis.parameters(), lr, (beta1,beta2)) #Discriminator\n",
    "\n",
    "#list(dis.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random noise for reference images\n",
    "z_ref = torch.rand(BATCH, NOISE_SIZE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = gen(z_ref)\n",
    "    \n",
    "# show images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "K = 1\n",
    "\n",
    "plotting = {'loss_d':[],\n",
    "           'loss_g':[]}\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0): #dataload\n",
    "        \n",
    "        image, _ = data  # data will be assigned to the generated image and input noise z\n",
    "\n",
    "        batch = image.size(0) #this insures it wont fail so it can take any batch size\n",
    "        \n",
    "        for k in range(K): #train discr twice\n",
    "\n",
    "            # 1) train Descriminator\n",
    "                optimiserD.zero_grad()\n",
    "                # 1a) Train on real images\n",
    "\n",
    "                output_real = dis(image)\n",
    "                labels = torch.ones_like(output_real)\n",
    "                \n",
    "                loss_real = criterion(output_real, labels) #output real is output of discr\n",
    "                loss_real.backward()\n",
    "\n",
    "                # 1b) Train on generated images\n",
    "                \n",
    "                # generate images\n",
    "                z = torch.rand(batch, NOISE_SIZE)\n",
    "                image_fake = gen(z)\n",
    "                \n",
    "                # remove gradients from generator\n",
    "                image_fake = image_fake.detach()\n",
    "                \n",
    "                # pass images through descriminator\n",
    "                output_fake = dis(image_fake)\n",
    "                labels = torch.zeros_like(output_fake)\n",
    "                \n",
    "                \n",
    "                loss_fake = criterion(output_fake, labels)\n",
    "                loss_fake.backward()\n",
    "\n",
    "                optimiserD.step()\n",
    "                \n",
    "                plotting['loss_d'].append((loss_real + loss_fake).item())\n",
    "            \n",
    "        # 2) train Generator\n",
    "        optimiserG.zero_grad()\n",
    "        \n",
    "        z = torch.rand(batch, NOISE_SIZE)\n",
    "        \n",
    "        image_gen = gen(z)\n",
    "        output_gen = dis(image_gen)\n",
    "        \n",
    "        \n",
    "        labels = torch.ones_like(output_gen)\n",
    "        loss_gen = criterion(output_gen, labels)\n",
    "        \n",
    "        loss_gen.backward()\n",
    "        optimiserG.step()\n",
    "        \n",
    "        plotting['loss_d'].append(loss_gen.item())\n",
    "            \n",
    "        #train/sample dataset/real images\n",
    "        #Forward propagation function for Discr\n",
    "        #Calculate loss then gradients for Discr\n",
    "            #BCELoss then mini batch SGD, mini batch size 128\n",
    "            \n",
    "        \n",
    "        #train/sample fake/Gen images\n",
    "        #Forward propagation function\n",
    "        #Calculate loss then gradients\n",
    "            #BCELoss then mini batch SGD, mini batch size 128\n",
    "        \n",
    "        #Then update Generator\n",
    "        #Forward prop, loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape 1:  torch.Size([5, 128, 32, 32])\n",
      "x shape 2:  torch.Size([5, 256, 16, 16])\n",
      "x shape 3:  torch.Size([5, 512, 8, 8])\n",
      "x shape 4:  torch.Size([5, 1024, 4, 4])\n",
      "x shape:  torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "#Discriminator step by step part\n",
    "\n",
    "#Input size\n",
    "B = 5\n",
    "C = 3\n",
    "H = 64\n",
    "W = 64\n",
    "#N = 3 #output noise dimension\n",
    "\n",
    "# Initialise input\n",
    "image = torch.randn(B, C, H, W)\n",
    "#z = z.view(B, C, H, W)\n",
    "\n",
    "# Create all layers\n",
    "conv  = nn.Conv2d(3, 128, 4, stride=2, padding=1, bias=False) #first layer, final output of original Gen, no Transpose\n",
    "conv1 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)\n",
    "conv2 = nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False)\n",
    "conv3 = nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False)\n",
    "conv4 = nn.Conv2d(1024, 1, 4, stride=1, padding=0, bias=False)\n",
    "\n",
    "bn1 = nn.BatchNorm2d(128) #capital B because it's part of the Class\n",
    "bn2 = nn.BatchNorm2d(256)\n",
    "bn3 = nn.BatchNorm2d(512)\n",
    "bn4 = nn.BatchNorm2d(1024)\n",
    "\n",
    "# Pass through each layer, Batch Norm \n",
    "#x = conv(image)\n",
    "x = (conv(image))\n",
    "x = F.leaky_relu(x, negative_slope=0.2)\n",
    "print(\"x shape 1: \", x.shape)\n",
    "x = bn2(conv1(x))\n",
    "x = F.leaky_relu(x, negative_slope=0.2)\n",
    "print(\"x shape 2: \", x.shape)\n",
    "x = bn3(conv2(x))\n",
    "x = F.leaky_relu(x, negative_slope=0.2)\n",
    "print(\"x shape 3: \", x.shape)\n",
    "x = bn4(conv3(x))\n",
    "x = F.leaky_relu(x, negative_slope=0.2)\n",
    "print(\"x shape 4: \", x.shape)\n",
    "x = conv4(x)\n",
    "\n",
    "# reshape x: [B, 1, 1, 1] -> [B, 1]\n",
    "x = x.view(x.size(0), 1)\n",
    "x = F.sigmoid(x)\n",
    "\n",
    "# x = x.view(-1, 1)\n",
    "# x = x.squeeze(-1)\n",
    "\n",
    "\n",
    "print(\"x shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(16,1,2,1)\n",
    "print(t.squeeze(-1).shape)\n",
    "print(t.squeeze(2).shape)\n",
    "\n",
    "print(t.view(-1, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.squeeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thing():\n",
    "    \n",
    "    def __init__(self, act=nn.LeakyReLU(negative_slope=0.2)):\n",
    "    \n",
    "        self.activation = act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = thing(act=nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(-10,10).float()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "lr = activation(t)\n",
    "\n",
    "plt.plot(t.tolist(), lr.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = torch.randn(100) #input z\n",
    "# self.z = torch.randn(4, 100) #(B,N)\n",
    "# self.z = z.view(4, 100, 1, 1) #(B,N,H,W)\n",
    "\n",
    "class Gen(nn.Module): #this is our class, the blueprint for creating an object to keep. Gen inherits from nn.Module\n",
    "    def __init__(self): #this is a init method; called upon whenever you create an instance of the class\n",
    "                    #using self means it connects this method to the instance of the class (Gen)\n",
    "        super(Gen, self).__init__()\n",
    "        # create all layers\n",
    "        #these are all our attributes, being assigned to parameters/arguments\n",
    "        self.conv =  nn.ConvTranspose2d(100, 1024, 4, stride=1, padding=0, bias=False) # transpose increases H,W size\n",
    "        self.conv1 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False) \n",
    "        self.conv2 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False) \n",
    "        self.conv3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv4 = nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(1024)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(3)\n",
    "        \n",
    "        # comparing with the multiclass classification tutorial with PyTorch, the hidden layers are the nn.Linear layers, which\n",
    "        # are not included here\n",
    "        \n",
    "    def forward(self, z): #this is our method, which belongs to the object. We want to be able to reference the instance of\n",
    "        # the particular object\n",
    "        \n",
    "        # pass through each layer \n",
    "        x = F.relu(self.bn1(self.conv(z)))\n",
    "        x = F.relu(self.bn2(self.conv1(x)))\n",
    "        x = F.relu(self.bn3(self.conv2(x)))\n",
    "        x = F.relu(self.bn4(self.conv3(x)))\n",
    "        x = F.relu(self.bn5(self.conv4(x)))\n",
    "        return x\n",
    "\n",
    "model = Gen() #this creates an instance of the class; the variable name (model) is = an instance of Gen\n",
    "\n",
    "z= torch.randn(BATCH,NOISE_SIZE)\n",
    "z = z.view(BATCH,NOISE_SIZE,1,1)\n",
    "\n",
    "output = model(z)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape:  torch.Size([4, 1024, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#Input size\n",
    "B = 4\n",
    "C = 1024\n",
    "H = 1\n",
    "W = 1\n",
    "\n",
    "N = 100 #input noise dimension Z\n",
    "\n",
    "#Initialise input\n",
    "z = torch.randn(B, N)\n",
    "z = z.view(B, N, 1, 1) #reshaping the input so it has H,W\n",
    "#print(\"z: \", z)\n",
    "\n",
    "\n",
    "#Create all layers\n",
    "conv =  nn.ConvTranspose2d(100, 1024, 4, stride=1, padding=0, bias=False) #input z needs to be reshaped and transposed\n",
    "conv1 = nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1, bias=False) #this is a class so you instantiate it, where the variables can change\n",
    "conv2 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False) #\n",
    "conv3 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)\n",
    "conv4 = nn.ConvTranspose2d(128, 3, 4, stride=2, padding=1, bias=False)\n",
    "\n",
    "bn1 = nn.BatchNorm2d(1024, momentum=None, affine=False)\n",
    "bn2 = nn.BatchNorm2d(512)\n",
    "bn3 = nn.BatchNorm2d(256)\n",
    "bn4 = nn.BatchNorm2d(128)\n",
    "bn5 = nn.BatchNorm2d(3)\n",
    "\n",
    "\n",
    "#Pass through each layer, batch norm first, reLU\n",
    "x = F.relu(bn1(conv(z)))\n",
    "#x = F.relu(bn2(conv1(x)))\n",
    "#x = F.relu(bn3(conv2(x)))\n",
    "#x = F.relu(bn4(conv3(x)))\n",
    "#x = F.relu(bn5(conv4(x)))\n",
    "print(\"Final shape: \", x.shape)\n",
    "\n",
    "# x2 = conv2(x1)\n",
    "# print(x2.shape)\n",
    "# x3 = conv3(x2)\n",
    "# print(x3.shape)\n",
    "# x4 = conv4(x3)\n",
    "# print(x4.shape)\n",
    "\n",
    "# # batch norm first then ReLU\n",
    "# m = nn.ReLU()\n",
    "# # anything with nn.Capital letter is a class you instantiate, so don't treat it as a function you pass variables to\n",
    "#  #applied to output of x1\n",
    "# inputx2 = bn1()\n",
    "\n",
    "# outputx2 = nn.BatchNorm2d(256) #output of x2\n",
    "# # inputx3 = m(x2)\n",
    "\n",
    "# outputx3 = nn.BatchNorm2d(128) #output of x3\n",
    "# # inputx4 = m(x3)\n",
    "\n",
    "# outputx4 = nn.BatchNorm2d(3)\n",
    "\n",
    "# #tanh\n",
    "# # torch.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all layers\n",
    "conv1 = nn.Conv2d(3,1,2)\n",
    "\n",
    "# input size \n",
    "B = 1\n",
    "C = 3\n",
    "H = 64\n",
    "W = 64\n",
    "\n",
    "# initialize input\n",
    "x = torch.randn(B, C, H, W)\n",
    "print(t.size())\n",
    "\n",
    "# pass through each layer \n",
    "x = conv1(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # With square kernels and equal stride\n",
    "m = nn.Conv2d(16, 33, 3, stride=2) #this needs to be initialised first to initialise\n",
    "# the weights, parameters in your network (so individual units/neurons). Remember: anything after nn. with a capital\n",
    "# is a class\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "# non-square kernels and unequal stride and with padding and dilation\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "\n",
    "\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With square kernels and equal stride\n",
    "m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "print(m)\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "output = m(input)\n",
    "\n",
    "# exact output size can be also specified as an argument\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "h.size()\n",
    "torch.Size([1, 16, 6, 6])\n",
    "output = upsample(h, output_size=input.size())\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1, 1, 1, 1)\n",
    "\n",
    "conv = nn.ConvTranspose2d(1, 1, 4, stride=1, padding=0, bias=False)\n",
    "\n",
    "conv(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(B, 100).view(B, 100, 1, 1)\n",
    "\n",
    "conv = nn.ConvTranspose2d(100, 1024, 4, stride=1, padding=0, bias=False)\n",
    "\n",
    "conv(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm2d(100)\n",
    "print(m)\n",
    "# Without Learnable Parameters\n",
    "m = nn.BatchNorm2d(100, affine=False)\n",
    "input = torch.randn(20, 100, 35, 45)\n",
    "\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    def __init__(self, channels):\n",
    "        self.channels = channels\n",
    "        self.weight = 100\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x + self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python basics\n",
    "\n",
    "# variables\n",
    "var1 = 1\n",
    "var2 = 'one'\n",
    "\n",
    "print(var1, var2)\n",
    "\n",
    "# functions\n",
    "\n",
    "def function_name(input1, input2):\n",
    "    return input1+input2\n",
    "\n",
    "print(\"Function:\", function_name(1,1))\n",
    "\n",
    "# classes\n",
    "print('\\n','Classes:','\\n')\n",
    "\n",
    "# create a Counter class inheriting from object class (build into python)\n",
    "class Counter(object):\n",
    "    \n",
    "    def __init__(self, start_count, count_step=1):\n",
    "        \n",
    "        self.count = start_count\n",
    "        self.count_step = count_step\n",
    "        \n",
    "    def step(self):\n",
    "        self.count += self.count_step\n",
    "        \n",
    "    def info(self):\n",
    "        print(self.count)\n",
    "        \n",
    "    def forward(self):\n",
    "        print(1)\n",
    "        \n",
    "    def __call__(self):\n",
    "        self.forward()\n",
    "        \n",
    "    \n",
    "days_counter = Counter(0, count_step=1)\n",
    "\n",
    "days_counter.info()\n",
    "days_counter.step()\n",
    "days_counter.step()\n",
    "days_counter.info()\n",
    "\n",
    "weeks_counter = Counter(25, count_step=7)\n",
    "\n",
    "# weeks_counter.info()\n",
    "print(weeks_counter.count)\n",
    "weeks_counter.step()\n",
    "weeks_counter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondsCounter(Counter):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SecondsCounter, self).__init__(0)\n",
    "        \n",
    "    def info(self):\n",
    "        print('Seconds:', self.count)\n",
    "        \n",
    "sc = SecondsCounter()\n",
    "\n",
    "sc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
